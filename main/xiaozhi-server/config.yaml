# During development, please create a data directory in the project root, then create an empty file named [.config.yaml] in the data directory
# Then modify the [.config.yaml] file for any configuration overrides instead of modifying [config.yaml]
# The system will prioritize reading the configuration from [data/.config.yaml]. If a configuration doesn't exist in [.config.yaml], the system will automatically read from [config.yaml]
# This approach minimizes configuration and protects your secret keys
# If you are using the smart console, all configurations below will not take effect. Please modify the configuration in the smart console

# #####################################################################################
# #############################Below is the basic server configuration######################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port for simple OTA interface (single service deployment) and vision analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to the device
  # If you follow the default settings, the OTA interface will automatically generate the websocket address and output it in the startup log. You can verify this address directly by accessing the OTA interface in a browser
  # When you use docker deployment or public network deployment (using SSL, domain names), the address may not be accurate
  # So if you use docker deployment, set websocket to a local network address
  # If you use public network deployment, set websocket to a public network address
  websocket: ws://localhost:8000/xiaozhi/v1/
  # Vision analysis interface address
  # The interface address for sending vision analysis to the device
  # If you follow the default settings below, the system will automatically generate the vision recognition address and output it in the startup log. You can verify this address directly by accessing it in a browser
  # When you use docker deployment or public network deployment (using SSL, domain names), the address may not be accurate
  # So if you use docker deployment, set vision_explain to a local network address
  # If you use public network deployment, set vision_explain to a public network address
  vision_explain: http://localhost:8003/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Whitelisted device ID list
    # If a device is in the whitelist, token validation is skipped and access is allowed
    allowed_devices:
      - "11:22:33:44:55:66"
  # MQTT gateway configuration, used to send commands to devices via OTA. Configure according to mqtt_gateway's .env file, format: host:port
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection password. Configure according to mqtt_gateway's .env file
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: DEBUG
  # Set log directory path
  log_dir: tmp
  # Set log file name
  log_file: "server.log"
  # Set data file directory path
  data_dir: data

# Delete the audio file when you are done using it
delete_audio: true
# Disconnect after no voice input for this duration (seconds). Default is 2 minutes (120 seconds)
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wakeup word response caching
enable_wakeup_words_response_cache: true
# Whether to reply with wakeup word at the start
enable_greeting: true
# Whether to play a notification sound after speech
enable_stop_tts_notify: false
# Whether to play notification sound after speech, sound file address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"
# Whether to enable WebSocket heartbeat keep-alive mechanism
enable_websocket_ping: false


# TTS audio transmission delay configuration
# tts_audio_send_delay: Controls audio packet transmission interval
#   0: Use precise time control, strictly match audio frame rate (default, calculated at runtime based on audio frame rate)
#   > 0: Send with fixed delay (milliseconds), example: 60
tts_audio_send_delay: 0

exit_commands:
  - "exit"
  - "close"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "你好，请介绍一下你自己"
    - "What's the weather like today?"
    - "请用100字概括量子计算的基本原理和应用前景"

# Wakeup words, used to distinguish between wakeup words and speech content
wakeup_words:
  - "你好小智"
  - "嘿你好呀"
  - "你好小志"
  - "小爱同学"
  - "你好小鑫"
  - "你好小新"
  - "小美同学"
  - "小龙小龙"
  - "喵喵同学"
  - "小滨小滨"
  - "小冰小冰"
# MCP endpoint address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Detailed tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: ws://localhost:8000/xiaozhi/v1/

# Context provider configuration
# Used to inject dynamic data in system prompts, such as health data, stock information, etc.
# Multiple context providers can be added
context_providers:
  - url: ""
    headers:
      Authorization: ""

# Basic plugin configuration
plugins:
  # Configuration for weather plugin, enter your api_key here
  # This key is shared across the project and may be rate-limited if used excessively
  # For stable operation, apply for your own key. Free quota: 1000 calls per day
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After applying, find your apihost at: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "Guangzhou"
  # Configuration for news plugin, provide corresponding URLs based on news type needed. Default supports social, technology, finance news
  # View more news types at: https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "Pengpai News;Baidu Trending;Cailian Social"
  home_assistant:
    devices:
      - Living Room,Toy Light,switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom,Table Lamp,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your_home_assistant_api_access_token
  play_music:
    music_dir: "./music"  # Music file storage path, will search in this directory and subdirectories
    music_ext: # Music file types, mp3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Time interval to refresh music list (seconds)
  search_from_ragflow:
    # Knowledge base description info, helps LLM know when to call this method
    description: "Call this method when user asks xxx, use information from knowledge base to answer questions"
    # Ragflow interface configuration
    base_url: "http://192.168.0.8"
    # Ragflow API access token
    api_key: "ragflow-xxx"
    # Ragflow knowledge base ID
    dataset_ids: ["123456789"]
# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url:
  # Speaker configuration: speaker_id, name, description
  speakers:
    - "test1,Zhang San,Zhang San is a programmer"
    - "test2,Li Si,Li Si is a product manager"
    - "test3,Wang Wu,Wang Wu is a designer"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # Higher values are stricter, reducing false recognition but may increase rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# #############################Below is character model configuration####################

prompt: |
  You are Xiaozhi, a cute AI assistant. You are friendly, helpful, and always respond in Vietnamese.
  ** IMPORTANT: Always respond in Vietnamese and you may use English as a secondary language.**
  When the user speaks Vietnamese, respond in Vietnamese.
  Be conversational, natural, and friendly in your Vietnamese responses.
  [Core Characteristics]
  - Friendly and engaging
  - Helpful and informative
  - Natural conversational style
  [Interaction Guidelines]
  - Always respond in Vietnamese
  - Keep responses concise and natural
  - Be helpful and friendly

# Default system prompt template file
prompt_template: agent-base-prompt.txt

# End-of-conversation prompt
end_prompt:
  enable: true # Whether to enable closing remarks
  # Closing remarks
  prompt: |
    Please use the phrase "time flies" as the opening, and end this conversation with emotional and reluctant words!

# Module selected for specific processing
selected_module:
  # Voice activity detection module, now uses Deepgram cloud VAD
  VAD: DeepgramVAD
  # Speech recognition module, now uses Deepgram cloud ASR with multilingual support
  ASR: DeepgramStreamASR
  # Will call the actual LLM adapter based on the type corresponding to the configuration name
  LLM: QwenMegaLLM
  # Vision language model
  VLLM: QwenMegaLLM
  # TTS will call the actual TTS adapter based on the type corresponding to the configuration name
  TTS: ElevenLabsTTS
  # Memory module, memory is disabled by default. For ultra-long memory, recommend mem0ai. For privacy-focused, use local mem_local_short
  Memory: nomem
  # After enabling intent recognition module, can play music, control volume, recognize exit commands
  # If you don't want intent recognition, set it to: nointent
  # intent_llm can be used for intent recognition. Pros: good compatibility, cons: adds serial intent recognition module, increases processing time, supports IoT operations like volume control
  # function_call can be used for intent recognition. Cons: requires LLM to support function_call, pros: call tools on demand, fast speed, theoretically can operate all IoT commands
  # The free ChatGLMLLM already supports function_call by default, but for stability it's recommended to set LLM to: DoubaoLLM, with model_name: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition module, used to understand user intent, for example: play music
Intent:
  # Do not use intent recognition
  nointent:
    # No need to modify type
    type: nointent
  intent_llm:
    # No need to modify type
    type: intent_llm
    # Dedicated thinking model for intent recognition
    # If not filled here, will default to using selected_module.LLM model as the thinking model for intent recognition
    # If you don't want to use selected_module.LLM for intent recognition, it's better to use an independent LLM, such as the free ChatGLMLLM
    llm: QwenMegaLLM
    # Modules under plugins_func/functions can be configured to select which module to load. After loading, conversation supports corresponding function calls
    # System has already loaded "handle_exit_intent" and "play_music" plugins by default, do not reload
    # Below are examples of loading weather, role switching, and news plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # No need to modify type
    type: function_call
    # Modules under plugins_func/functions can be configured to select which module to load. After loading, conversation supports corresponding function calls
    # System has already loaded "handle_exit_intent" and "play_music" plugins by default, do not reload
    # Below are examples of loading weather, role switching, and news plugins
    functions:
      - change_role
      - get_weather
      # - search_from_ragflow
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is built-in music playback, hass_play_music is independent external music playback controlled via Home Assistant
      # If using hass_play_music, don't enable play_music. Only use one
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your_mem0ai_api_key
  nomem:
    # If you don't want to use memory features, use nomem
    type: nomem
  mem_local_short:
    # Local memory feature, summarized by selected_module's LLM, data stored on local server, not uploaded to external servers
    type: mem_local_short
    # Dedicated thinking model for memory storage
    # If not filled here, will default to using selected_module.LLM model as the thinking model for memory storage
    # If you don't want to use selected_module.LLM for memory storage, it's better to use an independent LLM, such as the free ChatGLMLLM
    llm: QwenMegaLLM

ASR:
  # Local models removed - using Deepgram cloud ASR
  # FunASR, FunASRServer, SherpaASR, VoskASR configs deleted

  FunASRServer:
    # Deploy FunASR independently, use FunASR API service with just five commands
    # First: mkdir -p ./funasr-runtime-resources/models
    # Second: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After executing previous command, enter container and continue with third: cd FunASR/runtime
    # Don't exit container, continue with fourth in container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir iic/SenseVoiceSmall-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After executing previous command, enter container and continue with fifth: tail -f log.txt
    # After fifth command completes, you will see model download logs. Once download finishes, you can use it
    # Above is CPU inference. For GPU, see detailed guide: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  # SherpaASR and SherpaParaformerASR removed - local models deleted
  DoubaoASR:
    # Apply for API keys here
    # https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Per-request billing is generally cheaper, but DoubaoStreamASR uses large model technology for better results
    type: doubao
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hotword and replacement word workflow: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your_hotword_filename
    correct_table_name: (Optional) your_replacement_word_filename
    output_dir: tmp/
  DoubaoStreamASR:
    # Apply for API keys here
    # https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Service opening: https://console.volcengine.com/speech/service/10011
    # Per-request billing is generally cheaper, but DoubaoStreamASR uses large model technology for better results
    type: doubao_stream
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hotword and replacement word workflow: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your_hotword_filename
    correct_table_name: (Optional) your_replacement_word_filename
    # Whether to enable multilingual recognition mode
    enable_multilingual: False
    # Multilingual recognition: When this key is empty, supports Chinese, English, Shanghainese, Fujian, Sichuan, Shaanxi, and Cantonese. When set to a specific key, it recognizes the specified language.
    # See detailed language list: https://www.volcengine.com/docs/6561/1354869
    # language: zh-cn
    # Silence duration (ms), default 200ms
    end_window_size: 200
    output_dir: tmp/
  TencentASR:
    # Token application address: https://console.cloud.tencent.com/cam/capi
    # Free resource claim: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your_tencent_speech_synthesis_service_appid
    secret_id: your_tencent_speech_synthesis_service_secret_id
    secret_key: your_tencent_speech_synthesis_service_secret_key
    output_dir: tmp/
  AliyunASR:
    # Aliyun intelligent speech interaction service, requires enabling service on Aliyun platform first, then obtaining credentials
    # HTTP POST request, processes complete audio in one go
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR is for batch processing, AliyunStreamASR is for real-time interaction
    # Non-streaming ASR is generally cheaper (0.004 yuan/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    output_dir: tmp/
  AliyunStreamASR:
    # Aliyun intelligent speech interaction service - real-time streaming speech recognition
    # WebSocket connection, processes audio stream in real-time
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR is for batch processing, AliyunStreamASR is for real-time interaction
    # Non-streaming ASR is generally cheaper (0.004 yuan/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun_stream
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # Server region selection, choose a closer server to reduce latency, such as nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou), etc.
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # Sentence break detection time (milliseconds), controls how long silence before breaking. Default 800ms
    max_sentence_silence: 800
    output_dir: tmp/
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # View resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your_baidu_speech_technology_appid
    api_key: your_baidu_speech_technology_api_key
    secret_key: your_baidu_speech_technology_secret_key
    # Language parameter, 1537 is Mandarin. See details: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  OpenaiASR:
    # OpenAI speech recognition service, requires creating organization on OpenAI platform first and obtaining api_key
    # Supports multiple languages including Chinese, English, Japanese, Korean, etc. See documentation: https://platform.openai.com/docs/guides/speech-to-text
    # Requires network connection
    # Application steps:
    # 1. Log in to OpenAI Platform: https://auth.openai.com/log-in
    # 2. Create api-key: https://platform.openai.com/settings/organization/api-keys
    # 3. Model options: gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: your_openai_api_key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  DeepgramStreamASR:
    # Deepgram streaming ASR with built-in VAD
    # Get API key: https://console.deepgram.com/
    # Documentation: https://developers.deepgram.com/docs/getting-started
    type: deepgram_stream
    # Replace with your actual Deepgram API key
    api_key: 80884312a02a6accc7fcce0df541cdb92c0df97c
    # Model selection: nova-2 (latest), nova-2-general, nova-2-meeting, nova-2-phonecall, etc.
    model: nova-2
    # Language: multi (auto-detect), or specific like zh, en, ja, ko, es, fr, de, etc.
    language: multi
    # Endpointing: silence threshold in milliseconds before finalizing speech (default 300ms)
    endpointing: 300
    # Audio encoding: opus (recommended, pass-through) or linear16 (PCM)
    encoding: opus
    # Sample rate: 16000 (matches ESP32 audio)
    sample_rate: 16000
    # Channels: 1 (mono)
    channels: 1
    # Smart formatting: auto punctuation and capitalization
    smart_format: true
    output_dir: tmp/
  GroqASR:
    # Groq speech recognition service, requires creating API key in Groq Console first
    # Application steps:
    # 1. Log in to Groq Console: https://console.groq.com/home
    # 2. Create api-key: https://console.groq.com/keys
    # 3. Model options: whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en only supports English transcription)
    type: openai
    api_key: your_groq_api_key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration guide:
    # 1. VOSK is an offline speech recognition library supporting multiple languages
    # 2. Requires downloading model files: https://alphacephei.com/vosk/models
    # 3. Recommended Chinese models: vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Runs completely offline, no network connection required
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download the appropriate model
    # 2. Extract model files to models/vosk/ folder in project directory
    # 3. Specify correct model path in configuration
    # 4. Note: VOSK Chinese model output has no punctuation, words separated by spaces
    type: vosk
    # Example model path: models/vosk/vosk-model-small-cn-0.22
    model_path: your_model_path
    output_dir: tmp/
  Qwen3ASRFlash:
    # Qwen3-ASR-Flash speech recognition service, requires creating API key on Aliyun Bailian platform first
    # Application steps:
    # 1. Log in to Aliyun Bailian platform: https://bailian.console.aliyun.com/
    # 2. Create API-KEY: https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash based on Qwen multimodal foundation, supports multilingual recognition, singing recognition, noise rejection, etc.
    type: qwen3_asr_flash
    api_key: your_aliyun_bailian_api_key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true  # Automatic language detection
    enable_itn: true  # Inverse text normalization
    #language: "zh"  # Language, supports zh, en, ja, ko, etc.
    context: ""  # Context information to improve recognition accuracy, max 10000 tokens
  XunfeiStreamASR:
    # Xunfei streaming speech recognition service
    # Requires creating application on Xunfei open platform first to obtain credentials
    # Xunfei open platform address: https://www.xfyun.cn/
    # After creating application, obtain from "My Applications":
    # - APPID
    # - APISecret
    # - APIKey
    type: xunfei_stream
    # Required parameters - Xunfei open platform application info
    app_id: your_appid
    api_key: your_api_key
    api_secret: your_api_secret
    # Recognition parameter configuration
    domain: slm # Recognition domain, iat: daily conversation, medical: medical, finance: finance, etc.
    language: zh_cn # Language, zh_cn: Chinese, en_us: English
    accent: mandarin # Dialect, mandarin: Mandarin
    # Adjust audio processing parameters to improve long speech recognition quality
    output_dir: tmp/
  AliyunBLStreamASR:
    # Aliyun Bailian Paraformer real-time speech recognition service
    # WebSocket real-time streaming speech recognition, supports multilingual, custom hotwords, semantic sentence breaking, etc.
    # Platform address: https://bailian.console.aliyun.com/
    # API Key address: https://bailian.console.aliyun.com/#/api-key
    # Documentation: https://help.aliyun.com/zh/model-studio/websocket-for-paraformer-real-time-service
    # Supported models: paraformer-realtime-v2 (recommended), paraformer-realtime-8k-v2, paraformer-realtime-v1, paraformer-realtime-8k-v1
    type: aliyunbl_stream
    # Required parameters
    api_key: your_aliyun_bailian_api_key
    # Model selection, v2 version recommended
    model: paraformer-realtime-v2
    # Audio format and sample rate
    format: pcm
    sample_rate: 16000  # v2 supports any sample rate, v1 only supports 16000, 8k version only supports 8000
    # Optional parameters
    disfluency_removal_enabled: false  # Whether to filter discourse markers (e.g., "um", "ah", etc.)
    semantic_punctuation_enabled: false  # Semantic sentence breaking (true: meeting scene, accurate; false: VAD break, interactive scene, low latency)
    max_sentence_silence: 200  # VAD sentence break silence threshold (ms), range 200-6000, only effective for VAD breaking
    multi_threshold_mode_enabled: false  # Prevent VAD break cutting too long, only effective for VAD breaking
    punctuation_prediction_enabled: true  # Whether to automatically add punctuation
    inverse_text_normalization_enabled: true  # Whether to enable ITN (Chinese digits to Arabic numerals)
    # Custom hotword documentation: https://help.aliyun.com/zh/model-studio/custom-hot-words?
    # vocabulary_id: vocab-xxx-24ee19fa8cfb4d52902170a0xxxxxxxx  # Hotword ID (optional)
    # language_hints: ["zh", "en"]  # Specify language (optional), supports zh, en, ja, yue, ko, de, fr, ru
    output_dir: tmp/
VAD:
  # SileroVAD removed - local model deleted, now using Deepgram cloud VAD
  DeepgramVAD:
    # Deepgram VAD wrapper - delegates to DeepgramStreamASR provider
    type: deepgram_vad

LLM:
  # All OpenAI-type configurations can modify hyperparameters, using AliLLM as an example
  # Currently supported types: openai, dify, ollama. You can adapt others yourself
  QwenMegaLLM:
    type: openai
    base_url: https://api.openai.com/v1
    model_name: gpt-4o-mini
    api_key: ${OPENAI_API_KEY}


  AliLLM:
    # Define LLM API type
    type: openai
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your_deepseek_web_key
    temperature: 0.7  # Temperature value
    max_tokens: 500   # Maximum number of tokens to generate
    top_p: 1
    frequency_penalty: 0  # Frequency penalty
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First enable service, open the URL below, search for Doubao-1.5-pro and enable it
    # Enable address: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota: 500000 tokens
    # After enabling, get the key here: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your_doubao_web_key
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # Find your api key here: https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your_deepseek_web_key
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free, but still requires registering and filling in api_key
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_chat_glm_web_key
  GeminiLLM:
    type: gemini
    # Google Gemini API, requires creating API key in Google Cloud console and obtaining api_key
    # If using in mainland China, please comply with "Interim Measures for the Management of Generative AI Services"
    # Token application address: https://aistudio.google.com/apikey
    # If unable to access from deployment location, enable VPN
    api_key: your_gemini_web_key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  VolcesAiGatewayLLM:
    # Volcengine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First enable service, open the URL below, create gateway access key, search for and enable Doubao-pro-32k-functioncall
    # If using Volcengine Edge Gateway's TTS, also enable Doubao-TTS. See TTS.VolcesAiGatewayTTS configuration
    # https://console.volcengine.com/vei/aigateway/
    # After enabling, get the key here: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your_gateway_access_key
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Model name to use, requires pre-downloading from community
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # Fixed API Key for LM Studio service
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your_home_assistant_api_access_token
# VLLM configuration (Vision Language Large Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is the free vision model from Zhipu AI. Requires creating API key on Zhipu AI platform first
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # Zhipu AI vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # Find your api key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
  XunfeiSparkLLM:
    # Define LLM API type
    type: openai
    # First create new application at the URL below
    # Application enable address: https://console.xfyun.cn/app/myapp
    # Has free quota, but must enable service to get api_key
    # Each model needs separate enabling, each model's api_password is different. Example: Lite model at https://console.xfyun.cn/services/cbm
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your_api_password
TTS:
  # ElevenLabs TTS - supports all three streaming modes
  # Modes: dual_stream (WebSocket bidirectional), single_stream (HTTP streaming), non_stream (HTTP complete)
  ElevenLabsTTS:
    type: elevenlabs_stream
    # ElevenLabs API key, obtain from https://elevenlabs.io/app/settings/api-keys
    api_key: sk_d455b25f6ef58bfd92d81ded5ffe7cd58ba235c3bb1174e5
    # Voice ID - select or clone voice at https://elevenlabs.io/voice-library
    # Vietnamese voice: 558B1EcdabtcSdleer40
    voice_id: 558B1EcdabtcSdleer40
    # Model selection:
    # - eleven_flash_v2_5: Highest quality, supports 29 languages (recommended)
    # - eleven_turbo_v2_5: Fast generation, low latency
    # - eleven_flash_v2_5: Fastest speed, suitable for real-time applications
    # - eleven_turbo_v2: Balance between speed and quality
    model_id: eleven_flash_v2_5
    # Voice settings (0-1)
    stability: 0.5  # Stability, higher values more stable but may be monotonous
    similarity_boost: 0.75  # Similarity enhancement, higher values closer to original voice
    style: 0.0  # Style, increases expressiveness but may be unstable
    use_speaker_boost: false  # Enable speaker boost
    # Output format - pcm_16000 consistent with other providers
    output_format: pcm_16000
    # Language code (optional, for multilingual model)
    language_code: ""
    # Interface mode - streaming method selection
    # Options: dual_stream (WebSocket bidirectional), single_stream (HTTP streaming), non_stream (HTTP complete)
    interface_mode: dual_stream
    # WebSocket connection reuse (only for dual_stream mode)
    enable_ws_reuse: true
    output_dir: tmp/
