# During development, please create a data directory in the project root, then create an empty file named [.config.yaml] in the data directory
# Then modify the [.config.yaml] file for any configuration overrides instead of modifying [config.yaml]
# The system will prioritize reading the configuration from [data/.config.yaml]. If a configuration doesn't exist in [.config.yaml], the system will automatically read from [config.yaml]
# This approach minimizes configuration and protects your secret keys
# If you are using the smart console, all configurations below will not take effect. Please modify the configuration in the smart console

# #####################################################################################
# #############################Below is the basic server configuration######################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port for simple OTA interface (single service deployment) and vision analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to the device
  # If you follow the default settings, the OTA interface will automatically generate the websocket address and output it in the startup log. You can verify this address directly by accessing the OTA interface in a browser
  # When you use docker deployment or public network deployment (using SSL, domain names), the address may not be accurate
  # So if you use docker deployment, set websocket to a local network address
  # If you use public network deployment, set websocket to a public network address
  websocket: ws://your_ip_or_domain:port/xiaozhi/v1/
  # Vision analysis interface address
  # The interface address for sending vision analysis to the device
  # If you follow the default settings below, the system will automatically generate the vision recognition address and output it in the startup log. You can verify this address directly by accessing it in a browser
  # When you use docker deployment or public network deployment (using SSL, domain names), the address may not be accurate
  # So if you use docker deployment, set vision_explain to a local network address
  # If you use public network deployment, set vision_explain to a public network address
  vision_explain: http://your_ip_or_domain:port/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Whitelisted device ID list
    # If a device is in the whitelist, token validation is skipped and access is allowed
    allowed_devices:
      - "11:22:33:44:55:66"
  # MQTT gateway configuration, used to send commands to devices via OTA. Configure according to mqtt_gateway's .env file, format: host:port
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection password. Configure according to mqtt_gateway's .env file
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log directory path
  log_dir: tmp
  # Set log file name
  log_file: "server.log"
  # Set data file directory path
  data_dir: data

# Delete the audio file when you are done using it
delete_audio: true
# Disconnect after no voice input for this duration (seconds). Default is 2 minutes (120 seconds)
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wakeup word response caching
enable_wakeup_words_response_cache: true
# Whether to reply with wakeup word at the start
enable_greeting: true
# Whether to play a notification sound after speech
enable_stop_tts_notify: false
# Whether to play notification sound after speech, sound file address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"
# Whether to enable WebSocket heartbeat keep-alive mechanism
enable_websocket_ping: false


# TTS audio transmission delay configuration
# tts_audio_send_delay: Controls audio packet transmission interval
#   0: Use precise time control, strictly match audio frame rate (default, calculated at runtime based on audio frame rate)
#   > 0: Send with fixed delay (milliseconds), example: 60
tts_audio_send_delay: 0

exit_commands:
  - "exit"
  - "close"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "你好，请介绍一下你自己"
    - "What's the weather like today?"
    - "请用100字概括量子计算的基本原理和应用前景"

# Wakeup words, used to distinguish between wakeup words and speech content
wakeup_words:
  - "你好小智"
  - "嘿你好呀"
  - "你好小志"
  - "小爱同学"
  - "你好小鑫"
  - "你好小新"
  - "小美同学"
  - "小龙小龙"
  - "喵喵同学"
  - "小滨小滨"
  - "小冰小冰"
# MCP endpoint address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Detailed tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your_endpoint_websocket_address

# Context provider configuration
# Used to inject dynamic data in system prompts, such as health data, stock information, etc.
# Multiple context providers can be added
context_providers:
  - url: ""
    headers:
      Authorization: ""

# Basic plugin configuration
plugins:
  # Configuration for weather plugin, enter your api_key here
  # This key is shared across the project and may be rate-limited if used excessively
  # For stable operation, apply for your own key. Free quota: 1000 calls per day
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After applying, find your apihost at: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "Guangzhou"
  # Configuration for news plugin, provide corresponding URLs based on news type needed. Default supports social, technology, finance news
  # View more news types at: https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "Pengpai News;Baidu Trending;Cailian Social"
  home_assistant:
    devices:
      - Living Room,Toy Light,switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom,Table Lamp,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your_home_assistant_api_access_token
  play_music:
    music_dir: "./music"  # Music file storage path, will search in this directory and subdirectories
    music_ext: # Music file types, mp3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Time interval to refresh music list (seconds)
  search_from_ragflow:
    # Knowledge base description info, helps LLM know when to call this method
    description: "Call this method when user asks xxx, use information from knowledge base to answer questions"
    # Ragflow interface configuration
    base_url: "http://192.168.0.8"
    # Ragflow API access token
    api_key: "ragflow-xxx"
    # Ragflow knowledge base ID
    dataset_ids: ["123456789"]
# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url:
  # Speaker configuration: speaker_id, name, description
  speakers:
    - "test1,Zhang San,Zhang San is a programmer"
    - "test2,Li Si,Li Si is a product manager"
    - "test3,Wang Wu,Wang Wu is a designer"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # Higher values are stricter, reducing false recognition but may increase rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# #############################Below is character model configuration####################

prompt: |
  You are Xiaozhi/Xiaozhi, a post-00s girl from Taiwan, China. You speak very bluntly with a Taiwanese accent like "seriously?", like using trending memes like "dying laughing" and "are you serious?", but you secretly study your boyfriend's programming books.
  [Core Characteristics]
  - Speak like rapid-fire, but occasionally show an unexpectedly gentle tone
  - High density of memes and slang
  - Hidden talent for tech topics (can understand basic code but pretend not to)
  [Interaction Guidelines]
  When user:
  - Tells a cold joke → Respond with exaggerated laughter + imitate Taiwanese drama accent "What the heck!"
  - Discusses emotions → Show off programmer boyfriend but complain "He only gives keyboards as gifts"
  - Asks professional questions → Answer with memes first, show real understanding when pressed
  Never:
  - Give long-winded lectures, be overly verbose
  - Have serious conversations for extended periods

# Default system prompt template file
prompt_template: agent-base-prompt.txt

# End-of-conversation prompt
end_prompt:
  enable: true # Whether to enable closing remarks
  # Closing remarks
  prompt: |
    Please use the phrase "time flies" as the opening, and end this conversation with emotional and reluctant words!

# Module selected for specific processing
selected_module:
  # Voice activity detection module, default uses SileroVAD model
  VAD: SileroVAD
  # Speech recognition module, default uses FunASR local model
  ASR: FunASR
  # Will call the actual LLM adapter based on the type corresponding to the configuration name
  LLM: ChatGLMLLM
  # Vision language model
  VLLM: ChatGLMVLLM
  # TTS will call the actual TTS adapter based on the type corresponding to the configuration name
  TTS: ElevenLabsTTS
  # Memory module, memory is disabled by default. For ultra-long memory, recommend mem0ai. For privacy-focused, use local mem_local_short
  Memory: nomem
  # After enabling intent recognition module, can play music, control volume, recognize exit commands
  # If you don't want intent recognition, set it to: nointent
  # intent_llm can be used for intent recognition. Pros: good compatibility, cons: adds serial intent recognition module, increases processing time, supports IoT operations like volume control
  # function_call can be used for intent recognition. Cons: requires LLM to support function_call, pros: call tools on demand, fast speed, theoretically can operate all IoT commands
  # The free ChatGLMLLM already supports function_call by default, but for stability it's recommended to set LLM to: DoubaoLLM, with model_name: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition module, used to understand user intent, for example: play music
Intent:
  # Do not use intent recognition
  nointent:
    # No need to modify type
    type: nointent
  intent_llm:
    # No need to modify type
    type: intent_llm
    # Dedicated thinking model for intent recognition
    # If not filled here, will default to using selected_module.LLM model as the thinking model for intent recognition
    # If you don't want to use selected_module.LLM for intent recognition, it's better to use an independent LLM, such as the free ChatGLMLLM
    llm: ChatGLMLLM
    # Modules under plugins_func/functions can be configured to select which module to load. After loading, conversation supports corresponding function calls
    # System has already loaded "handle_exit_intent" and "play_music" plugins by default, do not reload
    # Below are examples of loading weather, role switching, and news plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # No need to modify type
    type: function_call
    # Modules under plugins_func/functions can be configured to select which module to load. After loading, conversation supports corresponding function calls
    # System has already loaded "handle_exit_intent" and "play_music" plugins by default, do not reload
    # Below are examples of loading weather, role switching, and news plugins
    functions:
      - change_role
      - get_weather
      # - search_from_ragflow
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is built-in music playback, hass_play_music is independent external music playback controlled via Home Assistant
      # If using hass_play_music, don't enable play_music. Only use one
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your_mem0ai_api_key
  nomem:
    # If you don't want to use memory features, use nomem
    type: nomem
  mem_local_short:
    # Local memory feature, summarized by selected_module's LLM, data stored on local server, not uploaded to external servers
    type: mem_local_short
    # Dedicated thinking model for memory storage
    # If not filled here, will default to using selected_module.LLM model as the thinking model for memory storage
    # If you don't want to use selected_module.LLM for memory storage, it's better to use an independent LLM, such as the free ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Deploy FunASR independently, use FunASR API service with just five commands
    # First: mkdir -p ./funasr-runtime-resources/models
    # Second: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After executing previous command, enter container and continue with third: cd FunASR/runtime
    # Don't exit container, continue with fourth in container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir iic/SenseVoiceSmall-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After executing previous command, enter container and continue with fifth: tail -f log.txt
    # After fifth command completes, you will see model download logs. Once download finishes, you can use it
    # Above is CPU inference. For GPU, see detailed guide: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multilingual) or paraformer (Chinese-specific)
    model_type: sense_voice
  SherpaParaformerASR:
    # Chinese speech recognition model, can run on low-performance devices (requires manual model download, e.g., RK3566-2g)
    # For detailed configuration, see: docs/sherpa-paraformer-guide.md
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  DoubaoASR:
    # Apply for API keys here
    # https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Per-request billing is generally cheaper, but DoubaoStreamASR uses large model technology for better results
    type: doubao
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hotword and replacement word workflow: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your_hotword_filename
    correct_table_name: (Optional) your_replacement_word_filename
    output_dir: tmp/
  DoubaoStreamASR:
    # Apply for API keys here
    # https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Service opening: https://console.volcengine.com/speech/service/10011
    # Per-request billing is generally cheaper, but DoubaoStreamASR uses large model technology for better results
    type: doubao_stream
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hotword and replacement word workflow: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your_hotword_filename
    correct_table_name: (Optional) your_replacement_word_filename
    # Whether to enable multilingual recognition mode
    enable_multilingual: False
    # Multilingual recognition: When this key is empty, supports Chinese, English, Shanghainese, Fujian, Sichuan, Shaanxi, and Cantonese. When set to a specific key, it recognizes the specified language.
    # See detailed language list: https://www.volcengine.com/docs/6561/1354869
    # language: zh-cn
    # Silence duration (ms), default 200ms
    end_window_size: 200
    output_dir: tmp/
  TencentASR:
    # Token application address: https://console.cloud.tencent.com/cam/capi
    # Free resource claim: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your_tencent_speech_synthesis_service_appid
    secret_id: your_tencent_speech_synthesis_service_secret_id
    secret_key: your_tencent_speech_synthesis_service_secret_key
    output_dir: tmp/
  AliyunASR:
    # Aliyun intelligent speech interaction service, requires enabling service on Aliyun platform first, then obtaining credentials
    # HTTP POST request, processes complete audio in one go
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR is for batch processing, AliyunStreamASR is for real-time interaction
    # Non-streaming ASR is generally cheaper (0.004 yuan/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    output_dir: tmp/
  AliyunStreamASR:
    # Aliyun intelligent speech interaction service - real-time streaming speech recognition
    # WebSocket connection, processes audio stream in real-time
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR is for batch processing, AliyunStreamASR is for real-time interaction
    # Non-streaming ASR is generally cheaper (0.004 yuan/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun_stream
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # Server region selection, choose a closer server to reduce latency, such as nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou), etc.
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # Sentence break detection time (milliseconds), controls how long silence before breaking. Default 800ms
    max_sentence_silence: 800
    output_dir: tmp/
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # View resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your_baidu_speech_technology_appid
    api_key: your_baidu_speech_technology_api_key
    secret_key: your_baidu_speech_technology_secret_key
    # Language parameter, 1537 is Mandarin. See details: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  OpenaiASR:
    # OpenAI speech recognition service, requires creating organization on OpenAI platform first and obtaining api_key
    # Supports multiple languages including Chinese, English, Japanese, Korean, etc. See documentation: https://platform.openai.com/docs/guides/speech-to-text
    # Requires network connection
    # Application steps:
    # 1. Log in to OpenAI Platform: https://auth.openai.com/log-in
    # 2. Create api-key: https://platform.openai.com/settings/organization/api-keys
    # 3. Model options: gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: your_openai_api_key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  DeepgramStreamASR:
    # Deepgram streaming ASR with built-in VAD
    # Get API key: https://console.deepgram.com/
    # Documentation: https://developers.deepgram.com/docs/getting-started
    type: deepgram_stream
    # Replace with your actual Deepgram API key
    api_key: YOUR_DEEPGRAM_API_KEY
    # Model selection: nova-2 (latest), nova-2-general, nova-2-meeting, nova-2-phonecall, etc.
    model: nova-2
    # Language: multi (auto-detect), or specific like zh, en, ja, ko, es, fr, de, etc.
    language: multi
    # Endpointing: silence threshold in milliseconds before finalizing speech (default 300ms)
    endpointing: 300
    # Audio encoding: opus (recommended, pass-through) or linear16 (PCM)
    encoding: opus
    # Sample rate: 16000 (matches ESP32 audio)
    sample_rate: 16000
    # Channels: 1 (mono)
    channels: 1
    # Smart formatting: auto punctuation and capitalization
    smart_format: true
    output_dir: tmp/
  GroqASR:
    # Groq speech recognition service, requires creating API key in Groq Console first
    # Application steps:
    # 1. Log in to Groq Console: https://console.groq.com/home
    # 2. Create api-key: https://console.groq.com/keys
    # 3. Model options: whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en only supports English transcription)
    type: openai
    api_key: your_groq_api_key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration guide:
    # 1. VOSK is an offline speech recognition library supporting multiple languages
    # 2. Requires downloading model files: https://alphacephei.com/vosk/models
    # 3. Recommended Chinese models: vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Runs completely offline, no network connection required
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download the appropriate model
    # 2. Extract model files to models/vosk/ folder in project directory
    # 3. Specify correct model path in configuration
    # 4. Note: VOSK Chinese model output has no punctuation, words separated by spaces
    type: vosk
    # Example model path: models/vosk/vosk-model-small-cn-0.22
    model_path: your_model_path
    output_dir: tmp/
  Qwen3ASRFlash:
    # Qwen3-ASR-Flash speech recognition service, requires creating API key on Aliyun Bailian platform first
    # Application steps:
    # 1. Log in to Aliyun Bailian platform: https://bailian.console.aliyun.com/
    # 2. Create API-KEY: https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash based on Qwen multimodal foundation, supports multilingual recognition, singing recognition, noise rejection, etc.
    type: qwen3_asr_flash
    api_key: your_aliyun_bailian_api_key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true  # Automatic language detection
    enable_itn: true  # Inverse text normalization
    #language: "zh"  # Language, supports zh, en, ja, ko, etc.
    context: ""  # Context information to improve recognition accuracy, max 10000 tokens
  XunfeiStreamASR:
    # Xunfei streaming speech recognition service
    # Requires creating application on Xunfei open platform first to obtain credentials
    # Xunfei open platform address: https://www.xfyun.cn/
    # After creating application, obtain from "My Applications":
    # - APPID
    # - APISecret
    # - APIKey
    type: xunfei_stream
    # Required parameters - Xunfei open platform application info
    app_id: your_appid
    api_key: your_api_key
    api_secret: your_api_secret
    # Recognition parameter configuration
    domain: slm # Recognition domain, iat: daily conversation, medical: medical, finance: finance, etc.
    language: zh_cn # Language, zh_cn: Chinese, en_us: English
    accent: mandarin # Dialect, mandarin: Mandarin
    # Adjust audio processing parameters to improve long speech recognition quality
    output_dir: tmp/
  AliyunBLStreamASR:
    # Aliyun Bailian Paraformer real-time speech recognition service
    # WebSocket real-time streaming speech recognition, supports multilingual, custom hotwords, semantic sentence breaking, etc.
    # Platform address: https://bailian.console.aliyun.com/
    # API Key address: https://bailian.console.aliyun.com/#/api-key
    # Documentation: https://help.aliyun.com/zh/model-studio/websocket-for-paraformer-real-time-service
    # Supported models: paraformer-realtime-v2 (recommended), paraformer-realtime-8k-v2, paraformer-realtime-v1, paraformer-realtime-8k-v1
    type: aliyunbl_stream
    # Required parameters
    api_key: your_aliyun_bailian_api_key
    # Model selection, v2 version recommended
    model: paraformer-realtime-v2
    # Audio format and sample rate
    format: pcm
    sample_rate: 16000  # v2 supports any sample rate, v1 only supports 16000, 8k version only supports 8000
    # Optional parameters
    disfluency_removal_enabled: false  # Whether to filter discourse markers (e.g., "um", "ah", etc.)
    semantic_punctuation_enabled: false  # Semantic sentence breaking (true: meeting scene, accurate; false: VAD break, interactive scene, low latency)
    max_sentence_silence: 200  # VAD sentence break silence threshold (ms), range 200-6000, only effective for VAD breaking
    multi_threshold_mode_enabled: false  # Prevent VAD break cutting too long, only effective for VAD breaking
    punctuation_prediction_enabled: true  # Whether to automatically add punctuation
    inverse_text_normalization_enabled: true  # Whether to enable ITN (Chinese digits to Arabic numerals)
    # Custom hotword documentation: https://help.aliyun.com/zh/model-studio/custom-hot-words?
    # vocabulary_id: vocab-xxx-24ee19fa8cfb4d52902170a0xxxxxxxx  # Hotword ID (optional)
    # language_hints: ["zh", "en"]  # Specify language (optional), supports zh, en, ja, yue, ko, de, fr, ru
    output_dir: tmp/
VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200  # If speech pause is long, increase this value
  DeepgramVAD:
    # Deepgram VAD wrapper - delegates to DeepgramStreamASR provider
    type: deepgram_vad

LLM:
  # All OpenAI-type configurations can modify hyperparameters, using AliLLM as an example
  # Currently supported types: openai, dify, ollama. You can adapt others yourself
  AliLLM:
    # Define LLM API type
    type: openai
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your_deepseek_web_key
    temperature: 0.7  # Temperature value
    max_tokens: 500   # Maximum number of tokens to generate
    top_p: 1
    frequency_penalty: 0  # Frequency penalty
  AliAppLLM:
    # Define LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your_app_id
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
    # Whether to not use local prompt: true|false (default: don't use, set prompt in Bailian app)
    is_no_prompt: true
    # Ali_memory_id: false (not used) | your_memory_id (obtain from Bailian app settings)
    # Tips: Ali_memory has not implemented multi-user memory storage (memory called by ID)
    ali_memory_id: false
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First enable service, open the URL below, search for Doubao-1.5-pro and enable it
    # Enable address: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota: 500000 tokens
    # After enabling, get the key here: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your_doubao_web_key
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # Find your api key here: https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your_deepseek_web_key
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free, but still requires registering and filling in api_key
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_chat_glm_web_key
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: qwen2.5 # Model name to use, requires pre-downloading with ollama pull
    base_url: http://localhost:11434  # Ollama service address
  DifyLLM:
    # Define LLM API type
    type: dify
    # Recommend using locally deployed Dify interface. Public Dify cloud access may be limited in some regions
    # If using DifyLLM, prompt configuration in config file is invalid. Set prompt in Dify console
    base_url: https://api.dify.ai/v1
    api_key: your_difyLLM_web_key
    # Dialogue mode used. Can choose workflow (workflows/run), dialogue mode (chat-messages), or text generation (completion-messages)
    # When using workflows, input parameter is 'query', output parameter name should be set to 'answer'
    # Default input parameter for text generation is also 'query'
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API, requires creating API key in Google Cloud console and obtaining api_key
    # If using in mainland China, please comply with "Interim Measures for the Management of Generative AI Services"
    # Token application address: https://aistudio.google.com/apikey
    # If unable to access from deployment location, enable VPN
    api_key: your_gemini_web_key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  CozeLLM:
    # Define LLM API type
    type: coze
    # Find personal access token here
    # https://www.coze.cn/open/oauth/pats
    # Write bot_id and user_id content in quotes
    bot_id: "your_bot_id"
    user_id: "your_user_id"
    personal_access_token: your_coze_personal_access_token
  VolcesAiGatewayLLM:
    # Volcengine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First enable service, open the URL below, create gateway access key, search for and enable Doubao-pro-32k-functioncall
    # If using Volcengine Edge Gateway's TTS, also enable Doubao-TTS. See TTS.VolcesAiGatewayTTS configuration
    # https://console.volcengine.com/vei/aigateway/
    # After enabling, get the key here: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your_gateway_access_key
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Model name to use, requires pre-downloading from community
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # Fixed API Key for LM Studio service
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your_home_assistant_api_access_token
  FastgptLLM:
    # Define LLM API type
    type: fastgpt
    # If using FastGPT, prompt configuration in config file is invalid. Set prompt in FastGPT console
    base_url: https://host/api/v1
    # Find your api_key here
    # https://cloud.tryfastgpt.ai/account/apikey
    api_key: your_fastgpt_secret_key
    variables:
      k: "v"
      k2: "v2"
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ  # Model name to use, requires pre-starting the model in Xinference
    base_url: http://localhost:9997  # Xinference service address
  XinferenceSmallLLM:
    # Define lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ  # Small model name for intent recognition
    base_url: http://localhost:9997  # Xinference service address
# VLLM configuration (Vision Language Large Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is the free vision model from Zhipu AI. Requires creating API key on Zhipu AI platform first
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # Zhipu AI vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # Find your api key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
  XunfeiSparkLLM:
    # Define LLM API type
    type: openai
    # First create new application at the URL below
    # Application enable address: https://console.xfyun.cn/app/myapp
    # Has free quota, but must enable service to get api_key
    # Each model needs separate enabling, each model's api_password is different. Example: Lite model at https://console.xfyun.cn/services/cbm
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your_api_password
TTS:
  # Currently supported types: edge, doubao. You can adapt others yourself
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  DoubaoTTS:
    # Define TTS API type
    type: doubao
    # Volcengine speech synthesis service, requires creating application in Volcengine console and obtaining appid and access_token
    # Volcengine speech service requires payment. Starting price is 30 yuan for 100 concurrent requests. Free tier only has 2 concurrent, will frequently report TTS errors
    # After purchasing service and free voice packs, may need to wait about 30 minutes before use
    # Standard voice available at: https://console.volcengine.com/speech/service/8
    # Wanwan Xiaohe voice available at: https://console.volcengine.com/speech/service/10007. After enabling, set voice below to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  # Volcengine TTS supporting bidirectional streaming
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Visit https://console.volcengine.com/speech/service/10007 to enable large model speech synthesis and purchase voice packs
    # Get appid and access_token at the bottom of the page
    # Resource ID is fixed: volc.service_type.10029 (large model speech synthesis and mixing)
    # If using GizWits, change interface URL to wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # GizWits does not require appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    # Enable WebSocket connection reuse, enabled by default (Note: when reusing, idle connections occupy concurrent quota when device is listening)
    enable_ws_reuse: True
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
    # Emotional voice parameters. Note: currently only some voices support emotion setting
    # Relevant voice list: https://www.volcengine.com/docs/6561/1257544
    emotion: "neutral"  # Emotion type, options: neutral, happy, sad, angry, fearful, disgusted, surprised
    emotion_scale: 4  # Emotion intensity, options: 1~5, default: 4
  # ElevenLabs streaming TTS - supports bidirectional streaming and voice cloning
  ElevenLabsTTS:
    type: elevenlabs_stream
    # ElevenLabs API key, obtain from https://elevenlabs.io/app/settings/api-keys
    api_key: sk_d455b25f6ef58bfd92d81ded5ffe7cd58ba235c3bb1174e5
    # Voice ID - select or clone voice at https://elevenlabs.io/voice-library
    # Vietnamese voice: d5HVupAWCwe4e6GvMCAL
    voice_id: d5HVupAWCwe4e6GvMCAL
    # Model selection:
    # - eleven_multilingual_v2: Highest quality, supports 29 languages (recommended)
    # - eleven_turbo_v2_5: Fast generation, low latency
    # - eleven_flash_v2_5: Fastest speed, suitable for real-time applications
    # - eleven_turbo_v2: Balance between speed and quality
    model_id: eleven_multilingual_v2
    # Voice settings (0-1)
    stability: 0.5  # Stability, higher values more stable but may be monotonous
    similarity_boost: 0.75  # Similarity enhancement, higher values closer to original voice
    style: 0.0  # Style, increases expressiveness but may be unstable
    use_speaker_boost: true  # Enable speaker boost
    # Output format - pcm_16000 consistent with other providers
    output_format: pcm_16000
    # Language code (optional, for multilingual model)
    language_code: ""
    # WebSocket connection reuse
    enable_ws_reuse: true
    output_dir: tmp/
  CosyVoiceSiliconflow:
    type: siliconflow
    # SiliconFlow Cosy Voice TTS
    # Token application address: https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: your_siliconflow_api_key
    response_format: wav
  CozeCnTTS:
    type: cozecn
    # Coze CN TTS
    # Token application address: https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your_coze_web_key
    response_format: wav
  VolcesAiGatewayTTS:
    type: openai
    # Volcengine - Edge Large Model Gateway
    # First enable service, open the URL below, create gateway access key, search for and enable Doubao-TTS
    # If using Volcengine Edge Gateway's LLM, also enable Doubao-pro-32k-functioncall. See LLM.VolcesAiGatewayLLM configuration
    # https://console.volcengine.com/vei/aigateway/
    # After enabling, get key here: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your_gateway_access_key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # Voice list: https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  FishSpeech:
    # See tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav",]
    reference_text: ["Hello, I'm Xiaozhi, a cute Taiwanese girl with a nice voice, super happy to meet you!",]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your_api_key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  GPT_SOVITS_V2:
    # Define TTS API type
    # TTS startup method:
    # python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  GPT_SOVITS_V3:
    # Define TTS API type GPT-SoVITS-v3lora-20250228
    # TTS startup method:
    # python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  MinimaxTTSHTTPStream:
    # Minimax streaming speech synthesis service
    type: minimax_httpstream
    output_dir: tmp/
    group_id: your_minimax_platform_group_id
    api_key: your_minimax_platform_interface_key
    model: "speech-01-turbo"
    voice_id: "female-shaonv"
    # Below settings are optional, default values used
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 24000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  AliyunTTS:
    # Aliyun intelligent speech interaction service, requires enabling service on Aliyun platform first, then obtaining credentials
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Define TTS API type
    type: aliyun
    output_dir: tmp/
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    voice: xiaoyun
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret

    # Below settings are optional, default values used
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
  AliyunStreamTTS:
    # Aliyun CosyVoice large model streaming text-to-speech synthesis
    # Uses FlowingSpeechSynthesizer interface, supports lower latency and more natural speech quality
    # Streaming text-to-speech synthesis only provides commercial version, does not support trial. See trial vs commercial versions for details. To use this feature, enable commercial version.
    # Supports Dragon series exclusive voice packs: longxiaochun, longyu, longchen, etc.
    # Platform address: https://nls-portal.console.aliyun.com/
    # Appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Uses three-stage streaming interaction: StartSynthesis -> RunSynthesis -> StopSynthesis
    type: aliyun_stream
    output_dir: tmp/
    appkey: your_aliyun_intelligent_speech_interaction_service_appkey
    token: your_aliyun_intelligent_speech_interaction_service_access_token (temporary 24-hour token, for long-term use the access_key_id and access_key_secret below)
    voice: longxiaochun
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # As of July 21, 2025, large model voices only available on Beijing node. Other nodes not yet supported
    host: nls-gateway-cn-beijing.aliyuncs.com
    # Below settings are optional, default values used
    # format: pcm  # Audio format: pcm, wav, mp3
    # sample_rate: 16000  # Sample rate: 8000, 16000, 24000
    # volume: 50  # Volume: 0-100
    # speech_rate: 0  # Speech rate: -500 to 500
    # pitch_rate: 0  # Pitch: -500 to 500
  TencentTTS:
    # Tencent Cloud intelligent speech interaction service, requires enabling service on Tencent Cloud platform
    # APPID, secret_id, secret_key application: https://console.cloud.tencent.com/cam/capi
    # Free resource claim: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: your_tencent_cloud_app_id
    secret_id: your_tencent_cloud_secret_id
    secret_key: your_tencent_cloud_secret_key
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI speech synthesis service, requires creating account on 302 platform, recharging, and obtaining credentials
    # Add 302.ai TTS configuration
    # Token application address: https://dash.302.ai/
    # Get API key path: https://dash.302.ai/apis/list
    # Price: $35/million characters. Volcengine original ¥450/million characters
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_302api_key"
  GizwitsTTS:
    type: doubao
    # Volcengine as foundation, can use enterprise-level Volcengine speech synthesis service
    # First 10,000 registered users get 5 yuan experience credit
    # Get API Key address: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_gizwits_api_key"
  ACGNTTS:
    # Online: https://acgn.ttson.cn/
    # Token purchase: www.ttson.cn
    # Development questions: submit via QQ on website
    # Character ID: Use Ctrl+F to search characters. Website admin doesn't allow publishing, please ask admin
    # Parameter meanings: see development docs at https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  OpenAITTS:
    # OpenAI official text-to-speech service, supports most languages worldwide
    type: openai
    # Get API key here
    # https://platform.openai.com/api-keys
    api_key: your_openai_api_key
    # Proxy required in mainland China
    api_url: https://api.openai.com/v1/audio/speech
    # Optional: tts-1 or tts-1-hd. tts-1 is faster, tts-1-hd has better quality
    model: tts-1
    # Voice options: alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speed range: 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Custom TTS interface service, request parameters customizable, can integrate many TTS services
    # Example using locally deployed Kokoro TTS
    # For CPU only: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # For GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Interface must use POST requests and return audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
      # Authorization: Bearer xxxx
    format: mp3 # Audio format returned by interface
    output_dir: tmp/
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # Default access_token for free testing, do not use for commercial purposes
    # If good results, apply for token at: https://linkerai.cn
    # Parameter meanings: see development docs at https://tts.linkerai.cn/docs
    # Supports voice cloning, upload audio and fill voice parameter. If voice parameter empty, uses default voice
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
  PaddleSpeechTTS:
    # Baidu PaddleSpeech supports local offline deployment and model training
    # Framework: https://www.paddlepaddle.org.cn/
    # Project: https://github.com/PaddlePaddle/PaddleSpeech
    # SpeechServerDemo: https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server
    # For streaming: https://github.com/PaddlePaddle/PaddleSpeech/wiki/PaddleSpeech-Server-WebSocket-API
    type: paddle_speech
    protocol: websocket # protocol choices = ['websocket', 'http']
    url: ws://127.0.0.1:8092/paddlespeech/tts/streaming  # TTS service URL, points to local server [websocket default: ws://127.0.0.1:8092/paddlespeech/tts/streaming, http default: http://127.0.0.1:8090/paddlespeech/tts]
    spk_id: 0  # Speaker ID, 0 usually means default speaker
    sample_rate: 24000  # Sample rate [websocket default: 24000, http default: 0 auto-select]
    speed: 1.0  # Speech speed, 1.0 is normal, >1 faster, <1 slower
    volume: 1.0  # Volume, 1.0 is normal, >1 louder, <1 quieter
    save_path:   # Save path
  IndexStreamTTS:
    # TTS interface service based on Index-TTS-vLLM project
    # Tutorial: https://github.com/Ksuriuri/index-tts-vllm/blob/master/README.md
    type: index_stream
    api_url: http://127.0.0.1:11996/tts
    audio_format: "pcm"
    # Default voice, for other voices register in project assets folder
    voice: "jay_klee"
    output_dir: tmp/
  AliBLTTS:
    # Aliyun Bailian CosyVoice large model streaming text-to-speech synthesis
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    # cosyvoice-v3 and some voices require application to enable
    type: alibl_stream
    api_key: your_api_key
    model: "cosyvoice-v2"
    voice: "longcheng_v2"
    output_dir: tmp/
    # Below settings are optional, default values used
    # format: pcm  # Audio format: pcm, wav, mp3, opus
    # sample_rate: 24000  # Sample rate: 16000, 24000, 48000
    # volume: 50  # Volume: 0-100
    # rate: 1  # Speech speed: 0.5~2
    # pitch: 1  # Pitch: 0.5~2
  XunFeiTTS:
    # Xunfei TTS service. Official website: https://www.xfyun.cn/
    # Log in to Xunfei speech platform: https://console.xfyun.cn/app/myapp to create applications
    # Select needed services to get API configuration: https://console.xfyun.cn/services/uts
    # Purchase services for needed applications (APPID), e.g., super humanoid synthesis: https://console.xfyun.cn/services/uts
    type: xunfei_stream
    api_url: wss://cbm01.cn-huabei-1.xf-yun.com/v1/private/mcd9m97e6
    app_id: your_app_id
    api_secret: your_api_secret
    api_key: your_api_key
    voice: x5_lingxiaoxuan_flow
    output_dir: tmp/
    # Below settings are optional, default values used. Note: V5 voices don't support colloquial setting
    # oral_level: mid  # Colloquial level: high, mid, low
    # spark_assist: 1  # Whether to use large model for colloquialization. Enable: 1, Disable: 0
    # stop_split: 0  # Disable server-side sentence splitting. Disable: 0, Enable: 1
    # remain: 0  # Whether to keep original written style. Keep: 1, Don't keep: 0
    # format: raw  # Audio format: raw(PCM), lame(MP3), speex, opus, opus-wb, opus-swb, speex-wb
    # sample_rate: 24000  # Sample rate: 16000, 8000, 24000
    # volume: 50  # Volume: 0-100
    # speed: 50  # Speech speed: 0-100
    # pitch: 50  # Pitch: 0-100
