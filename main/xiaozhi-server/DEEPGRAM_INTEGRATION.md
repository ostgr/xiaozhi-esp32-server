# Deepgram Integration Guide

## Overview

This guide explains how to use Deepgram for both Voice Activity Detection (VAD) and Automatic Speech Recognition (ASR) in xiaozhi-server, replacing the local SileroVAD and FunASR models with Deepgram's cloud-based streaming API.

## Features

âœ… **Single WebSocket Connection** - Both VAD and ASR use one connection (efficient, low latency)
âœ… **Opus Pass-Through** - No audio decoding needed (lower CPU, lower bandwidth)
âœ… **Multilingual Support** - Nova-2 model with automatic language detection (Chinese, English, Japanese, Korean, etc.)
âœ… **Built-in VAD** - Deepgram's SpeechStarted events provide voice activity detection
âœ… **Configurable Endpointing** - Adjustable silence threshold (default 300ms)
âœ… **Final Results Only** - No interim results for cleaner output
âœ… **Easy Rollback** - Switch back to local models anytime via config

## Architecture

### Combined Provider Approach
- **DeepgramStreamASRProvider**: Main implementation with WebSocket connection
- **DeepgramVADProvider**: Lightweight wrapper that delegates to ASR's VAD events
- **DeepgramConnectionManager**: Shared connection manager for lifecycle management

### How It Works
```
ESP32 Audio (Opus)
    â†“
VAD Check (Deepgram VAD wrapper)
    â†“
ASR Provider establishes WebSocket
    â†“
Stream Opus packets to Deepgram
    â†“
Receive SpeechStarted event â†’ VAD active
    â†“
Continue streaming audio
    â†“
Receive Final Transcript + UtteranceEnd
    â†“
Close connection, return transcript
```

## Installation

### 1. Get Deepgram API Key

1. Sign up at https://console.deepgram.com/
2. Create a new API key
3. Copy the key (starts with something like `abc123...`)

### 2. Configure xiaozhi-server

Edit `config.yaml`:

```yaml
# Update selected modules (around line 213)
selected_module:
  VAD: DeepgramVAD        # Change from SileroVAD
  ASR: DeepgramStreamASR  # Change from FunASR
  # ... keep other modules unchanged

# Update API key (around line 430)
ASR:
  DeepgramStreamASR:
    api_key: YOUR_ACTUAL_DEEPGRAM_API_KEY  # â† Replace with your key!
    model: nova-2                           # Latest model
    language: multi                         # Auto language detection
    endpointing: 300                        # 300ms silence threshold
    encoding: opus                          # Pass-through (efficient)
    sample_rate: 16000                      # Matches ESP32
    channels: 1                             # Mono
    smart_format: true                      # Auto punctuation
```

### 3. Start the Server

```bash
cd server-ai-py/main/xiaozhi-server
python app.py
```

## Configuration Options

### Model Selection
```yaml
model: nova-2              # Latest general-purpose model (recommended)
# model: nova-2-general    # Alternative general model
# model: nova-2-meeting    # Optimized for meetings
# model: nova-2-phonecall  # Optimized for phone calls
```

### Language Options
```yaml
language: multi            # Auto-detect (Chinese, English, Japanese, Korean, etc.)
# language: zh             # Force Chinese
# language: en             # Force English
# language: ja             # Force Japanese
# language: ko             # Force Korean
```

### Endpointing (Silence Threshold)
```yaml
endpointing: 300           # 300ms default (balanced)
# endpointing: 200         # 200ms (faster, may cut off)
# endpointing: 500         # 500ms (slower, more complete)
```

Adjust based on your use case:
- **Quick commands** (e.g., "turn on lights"): 200ms
- **Normal conversation**: 300ms (recommended)
- **Long-form speech**: 500ms

### Audio Encoding
```yaml
encoding: opus             # Pass-through (recommended, most efficient)
# encoding: linear16       # PCM (requires decoding, higher bandwidth)
```

## Testing

### Simple Test (Recommended)
```bash
python test_deepgram_simple.py
```

Expected output:
```
âœ“ PASS   Deepgram SDK
âœ“ PASS   File Structure
âœ“ PASS   Python Syntax
âœ“ PASS   Config Syntax
âœ“ PASS   Class Structure

Results: 5/5 tests passed
ğŸ‰ All standalone tests passed!
```

### Full Integration Test (Requires Full Environment)
```bash
python test_deepgram_providers.py
```

## File Structure

```
server-ai-py/main/xiaozhi-server/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ deepgram_client.py        # Connection manager
â”‚   â”‚   â”œâ”€â”€ asr/
â”‚   â”‚   â”‚   â””â”€â”€ deepgram_stream.py        # ASR provider
â”‚   â”‚   â””â”€â”€ vad/
â”‚   â”‚       â””â”€â”€ deepgram_vad.py           # VAD wrapper
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ asr.py                        # ASR factory (auto-registers)
â”‚       â””â”€â”€ vad.py                        # VAD factory (auto-registers)
â”œâ”€â”€ config.yaml                           # Main configuration
â”œâ”€â”€ requirements.txt                      # Dependencies (includes deepgram-sdk)
â”œâ”€â”€ test_deepgram_simple.py              # Simple test script
â”œâ”€â”€ test_deepgram_providers.py           # Full test script
â””â”€â”€ DEEPGRAM_INTEGRATION.md              # This file
```

## Monitoring & Debugging

### Log Levels
Check logs in `tmp/server.log` or console output:

```
[INFO] Initialized Deepgram ASR: model=nova-2, language=multi, endpointing=300ms
[INFO] Deepgram connection established in 234ms
[DEBUG] Sending 10 buffered audio packets
[DEBUG] Deepgram detected speech start
[INFO] Deepgram transcript: ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ
[DEBUG] Deepgram detected utterance end
```

### Performance Metrics
- **Connection time**: 200-500ms (one-time per utterance)
- **Streaming latency**: ~100ms (similar to local ASR)
- **Total latency**: ~1s from speech stop to transcript

### Common Issues

**Issue: "Invalid Deepgram API key"**
- Check that you replaced `YOUR_DEEPGRAM_API_KEY` in config.yaml
- Verify key is active in Deepgram console

**Issue: Connection timeout**
- Check internet connectivity
- Verify firewall allows outbound WebSocket connections
- Check Deepgram service status: https://status.deepgram.com/

**Issue: No transcription results**
- Check audio format (should be Opus, 16kHz, mono)
- Increase endpointing timeout (e.g., 500ms)
- Enable DEBUG logging to see WebSocket events

**Issue: Transcription cuts off mid-sentence**
- Increase `endpointing` value (e.g., from 300 to 500ms)
- Check that audio packets arrive continuously

## Rollback to Local Models

If you need to switch back to local models:

```yaml
selected_module:
  VAD: SileroVAD       # Local VAD
  ASR: FunASR          # Local ASR (or FunASRServer)
```

Restart the server - no code changes needed!

## Cost Optimization

### Deepgram Pricing
- Pay-as-you-go: ~$0.0043 per minute
- Pre-recorded audio: ~$0.0125 per minute
- See latest pricing: https://deepgram.com/pricing

### Tips to Reduce Costs
1. **Use Opus encoding** - Lower bandwidth = lower costs
2. **Tune endpointing** - Shorter silence detection = less streaming time
3. **Close idle connections** - Implemented automatically after utterance
4. **Use specific languages** - Skip auto-detection if you know the language

## Advanced Configuration

### Connection Pooling (Future Enhancement)
For high-concurrency deployments, you may want to implement connection pooling to reuse WebSocket connections across multiple conversations.

### Hybrid VAD (Future Enhancement)
For ultra-low latency, you could implement a two-stage VAD:
1. **Local pre-VAD** (SileroVAD): Fast local check (<50ms)
2. **Cloud VAD** (Deepgram): Authoritative VAD for endpointing

This would trigger Deepgram connection faster while maintaining accurate endpointing.

## Troubleshooting

### Enable Debug Logging
Edit `config.yaml`:
```yaml
log:
  log_level: DEBUG  # Change from INFO to DEBUG
```

### Test WebSocket Connection Manually
```python
import asyncio
from deepgram import DeepgramClient, LiveOptions, LiveTranscriptionEvents

async def test():
    client = DeepgramClient(api_key="YOUR_API_KEY")
    connection = client.listen.asynclive.v("1")

    connection.on(LiveTranscriptionEvents.Open, lambda _: print("Connected!"))
    connection.on(LiveTranscriptionEvents.Transcript, lambda r: print(f"Got: {r}"))

    options = LiveOptions(
        model="nova-2",
        language="multi",
        encoding="opus",
        sample_rate=16000,
    )

    await connection.start(options)
    print("Connection established!")

asyncio.run(test())
```

## Support

- **Deepgram Documentation**: https://developers.deepgram.com/
- **Deepgram Support**: https://developers.deepgram.com/support/
- **xiaozhi-server Issues**: https://github.com/[your-repo]/issues

## License

This integration follows the same license as xiaozhi-server.

## Credits

- **Deepgram**: Cloud speech recognition platform
- **xiaozhi-esp32-server**: ESP32 voice AI framework
- Implementation by: Claude Code Agent
